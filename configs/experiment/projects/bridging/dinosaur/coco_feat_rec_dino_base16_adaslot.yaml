# @package _global_
defaults:
  - /conditioning/random@models.conditioning
  - /experiment/projects/bridging/dinosaur/_base_feature_recon_gumbel
  - /neural_networks/mlp@models.object_decoder.decoder
  - /dataset: coco
  - /experiment/projects/bridging/dinosaur/_preprocessing_coco_dino_feature_recon_ccrop
  - /experiment/projects/bridging/dinosaur/_metrics_coco
  - /metrics/tensor_statistic@training_metrics.hard_keep_decision
  - /metrics/tensor_statistic@training_metrics.slots_keep_prob
  - _self_

# The following parameters assume training on 8 GPUs, leading to an effective batch size of 64.
trainer:
  gpus: 8
  max_steps: 500000
  max_epochs: null
  strategy: ddp

dataset:
  num_workers: 4
  batch_size: 8

models:
  _target_: ocl.models.image_grouping_adaslot.GroupingImgGumbel
  conditioning:
    n_slots: 33
    object_dim: 256

  feature_extractor:
    model_name: vit_base_patch16_224_dino
    pretrained: true
    freeze: true

  perceptual_grouping:
    input_dim: 768
    low_bound: 0

  object_decoder:
    _target_: ocl.decoding.PatchDecoderGumbelV1
    decoder:
      features: [2048, 2048, 2048]
    left_mask_path: None
    mask_type: mask_normalized

losses:
  sparse_penalty:
    _target_: ocl.losses.SparsePenalty
    linear_weight: 0.1
    quadratic_weight: 0.0
    quadratic_bias: 0.5
    input_path: hard_keep_decision

# outputs["hard_keep_decision"] = perceptual_grouping_output["hard_keep_decision"]
# outputs["slots_keep_prob"]
training_metrics:
  hard_keep_decision:
    path: hard_keep_decision
    reduction: sum
  
  slots_keep_prob:
    path: slots_keep_prob
    reduction: mean

load_model_weight: /home/ubuntu/GitLab/bags-of-tricks/object-centric-learning-models/outputs/projects/bridging/dinosaur/coco_feat_rec_dino_base16.yaml/2023-05-02_16-57-16/lightning_logs/version_0/checkpoints/epoch=95-step=177408.ckpt