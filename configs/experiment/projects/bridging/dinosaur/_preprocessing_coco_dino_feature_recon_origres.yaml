# @package _global_
defaults:
  - /plugins/data_preprocessing@plugins.03a_preprocessing
  - /plugins/multi_element_preprocessing@plugins.03b_preprocessing

plugins:
  03a_preprocessing:
    evaluation_fields:
      - image
      - instance_mask
      - instance_category
    evaluation_transform:
      _target_: torchvision.transforms.Compose
      transforms:
        - _target_: ocl.preprocessing.InstanceMasksToDenseMasks
        - _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask
        # Drop instance_category again as some images do not contain it
        - "${lambda_fn:'lambda data: {k: v for k, v in data.items() if k != \"instance_category\"}'}"
        - _target_: ocl.preprocessing.AddEmptyMasks
          mask_keys:
            - instance_mask
            - segmentation_mask

  03b_preprocessing:
    training_transforms:
      image:
        _target_: torchvision.transforms.Compose
        transforms:
          - _target_: torchvision.transforms.ToTensor
          - _target_: torchvision.transforms.Resize
            _convert_: partial
            size: [224, 224]
            interpolation: "${torchvision_interpolation_mode:BICUBIC}"
          - "${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}"  # Bicubic interpolation can get out of range
          - _target_: torchvision.transforms.RandomHorizontalFlip
          - _target_: torchvision.transforms.Normalize
            mean: [0.485, 0.456, 0.406]
            std: [0.229, 0.224, 0.225]
    evaluation_transforms:
      image:
        _target_: torchvision.transforms.Compose
        transforms:
          - _target_: torchvision.transforms.ToTensor
          - _target_: torchvision.transforms.Resize
            _convert_: partial
            size: [224, 224]
            interpolation: "${torchvision_interpolation_mode:BICUBIC}"
          - "${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}"  # Bicubic interpolation can get out of range
          - _target_: torchvision.transforms.Normalize
            mean: [0.485, 0.456, 0.406]
            std: [0.229, 0.224, 0.225]
      instance_mask:
        _target_: torchvision.transforms.Compose
        transforms:
          - _target_: ocl.preprocessing.DenseMaskToTensor
          - _target_: ocl.preprocessing.ResizeNearestExact
            _convert_: partial
            size: [224, 224]
      segmentation_mask:
        _target_: torchvision.transforms.Compose
        transforms:
          - _target_: ocl.preprocessing.DenseMaskToTensor
          - _target_: ocl.preprocessing.ResizeNearestExact
            _convert_: partial
            size: [224, 224]
