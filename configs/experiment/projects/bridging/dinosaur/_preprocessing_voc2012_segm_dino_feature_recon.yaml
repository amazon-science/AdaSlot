# @package _global_
defaults:
  - /plugins/multi_element_preprocessing@plugins.02a_format_consistency
  - /plugins/data_preprocessing@plugins.02b_format_consistency
  - /plugins/data_preprocessing@plugins.03a_preprocessing
  - /plugins/multi_element_preprocessing@plugins.03b_preprocessing

plugins:
  # Make VOC2012 cosistent with COCO.
  02a_format_consistency:
    evaluation_transforms:
      # Convert to one-hot encoding.
      segmentation-instance:
        _target_: ocl.preprocessing.IntegerToOneHotMask

  02b_format_consistency:
    evaluation_fields:
      - "segmentation-instance"
      - "segmentation-class"
      - "image"
    evaluation_transform:
      _target_: torchvision.transforms.Compose
      transforms:
        # Create segmentation mask.
        - _target_: ocl.preprocessing.VOCInstanceMasksToDenseMasks
          instance_mask_key: segmentation-instance
          class_mask_key: segmentation-class
          classes_key: instance_category
        - _target_: ocl.preprocessing.RenameFields
          mapping:
            segmentation-instance: instance_mask
  03a_preprocessing:
    evaluation_fields:
      - image
      - instance_mask
      - instance_category
    evaluation_transform:
      _target_: torchvision.transforms.Compose
      transforms:
        # This is not needed for VOC.
        # - _target_: ocl.preprocessing.InstanceMasksToDenseMasks
        - _target_: ocl.preprocessing.AddSegmentationMaskFromInstanceMask
        # Drop instance_category again as some images do not contain it
        - "${lambda_fn:'lambda data: {k: v for k, v in data.items() if k != \"instance_category\"}'}"
        - _target_: ocl.preprocessing.AddEmptyMasks
          mask_keys:
            - instance_mask
            - segmentation_mask

  03b_preprocessing:
    training_transforms:
      image:
        _target_: torchvision.transforms.Compose
        transforms:
          - _target_: torchvision.transforms.ToTensor
          - _target_: torchvision.transforms.Resize
            size: 224
            interpolation: "${torchvision_interpolation_mode:BICUBIC}"
          - "${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}"  # Bicubic interpolation can get out of range
          - _target_: torchvision.transforms.RandomCrop
            size: 224
          - _target_: torchvision.transforms.RandomHorizontalFlip
          - _target_: torchvision.transforms.Normalize
            mean: [0.485, 0.456, 0.406]
            std: [0.229, 0.224, 0.225]
    evaluation_transforms:
      image:
        _target_: torchvision.transforms.Compose
        transforms:
          - _target_: torchvision.transforms.ToTensor
          - _target_: torchvision.transforms.Resize
            size: 224
            interpolation: "${torchvision_interpolation_mode:BICUBIC}"
          - "${lambda_fn:'lambda image: image.clamp(0.0, 1.0)'}"  # Bicubic interpolation can get out of range
          - _target_: torchvision.transforms.CenterCrop
            size: 224
          - _target_: torchvision.transforms.Normalize
            mean: [0.485, 0.456, 0.406]
            std: [0.229, 0.224, 0.225]
      instance_mask:
        _target_: torchvision.transforms.Compose
        transforms:
          - _target_: ocl.preprocessing.DenseMaskToTensor
          - _target_: ocl.preprocessing.ResizeNearestExact
            size: 224
          - _target_: torchvision.transforms.CenterCrop
            size: 224
      segmentation_mask:
        _target_: torchvision.transforms.Compose
        transforms:
          - _target_: ocl.preprocessing.DenseMaskToTensor
          - _target_: ocl.preprocessing.ResizeNearestExact
            size: 224
          - _target_: torchvision.transforms.CenterCrop
            size: 224
